{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1536cc-91cd-45fb-8205-b2346eaec2f7",
   "metadata": {},
   "source": [
    "# Embedded ML Lab - Challenge (Camera example)\n",
    "\n",
    "This is an example notebook for the camera usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68e44cdf-8743-4dc4-8ad0-77ec15525b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from faf.tensorrt import CameraTensorrtDisplay\n",
    "from faf.utils.camera import CameraDisplay\n",
    "import time\n",
    "import cv2\n",
    "from faf.tinyyolov2 import TinyYoloV2\n",
    "import torch\n",
    "from faf.inference import InferenceModel\n",
    "from torchvision.transforms import ToTensor\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from faf.utils.yolo import filter_boxes_separate, nms, filter_boxes\n",
    "from faf.visualization import visualize_result, draw_bbox_opencv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a044cdf-d482-4f67-8b09-b4326bc94429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ec75f-8f90-468c-8dec-1d7ad0859abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnxruntime.get_available_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c1a7c-ce95-4386-b1a4-8cfcf9d48e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/onnx/onnx-tensorrt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7e5d98-a493-4242-bdba-d7f98732c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TinyYoloV2.from_saved_state_dict(\"../weights/voc_pretrained.pt\", use_constant_padding=True, use_leaky_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9571315-cbe4-4dce-a736-0f2fd15a7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c224144-46b0-435f-9456-3eab9ba647a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "x = torch.randn(1, 3, 320, 320, requires_grad=True)\n",
    "\n",
    "out_path = os.path.join(\"./\", \"test.onnx\")\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    net,  # model being run\n",
    "    x,  # model input (or a tuple for multiple inputs)\n",
    "    out_path,  # where to save the model (can be a file or file-like object)\n",
    "    export_params=True,  # store the trained parameter weights inside the model file\n",
    "    opset_version=10,  # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    keep_initializers_as_inputs=True,\n",
    "    input_names=[\"input\"],  # the model's input names\n",
    "    output_names=[\"output\"],  # the model's output names\n",
    "    # dynamic_axes={\n",
    "    #     \"input\": {0: \"batch_size\"},\n",
    "    #     \"output\": {0: \"batch_size\"},\n",
    "    # },  # variable length axes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255baf4-8ed0-40a2-b913-e45a0d6fcd99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnx_tensorrt.backend as backend\n",
    "import numpy as np\n",
    "model = onnx.load(\"../weights/simplified.onnx\")\n",
    "\n",
    "print(onnx.checker.check_model(model))\n",
    "engine = backend.prepare(model, device='CUDA:0')\n",
    "input_data = np.random.random(size=(1, 3, 320, 320)).astype(np.float32)\n",
    "output_data = engine.run(input_data)[0]\n",
    "print(output_data)\n",
    "print(output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e2f85-1c2a-4982-9990-2d844e10f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = onnxruntime.get_available_providers()\n",
    "assert 'CUDAExecutionProvider' in providers, \"CUDAExecution Provider is not available.\"\n",
    "\n",
    "# Create a session options object\n",
    "session_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Set the GPU memory limit (in bytes, for example 0.5 GB here)\n",
    "gpu_mem_limit = int(0.5 * 1024 * 1024 * 1024)\n",
    "\n",
    "# Configuration for the CUDA Execution Provider\n",
    "cuda_provider_options = {\n",
    "    \"device_id\": \"0\",  # Assuming using GPU device 0\n",
    "    \"gpu_mem_limit\": gpu_mem_limit,\n",
    "    \"arena_extend_strategy\": \"kSameAsRequested\",\n",
    "    \"cudnn_conv_algo_search\": \"HEURISTIC\",\n",
    "    # \"cudnn_conv_use_max_workspace\": '1'\n",
    "}\n",
    "\n",
    "# Create the session with the model and the configured session options\n",
    "# Replace 'your_model.onnx' with the path to your ONNX model\n",
    "ort_session = onnxruntime.InferenceSession(\n",
    "    \"../weights/simplified.onnx\", \n",
    "    sess_options=session_options, \n",
    "    providers=['CUDAExecutionProvider'], \n",
    "    provider_options=[cuda_provider_options]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e7afd-2de3-429c-948d-53cd69097e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = TinyYoloV2.from_saved_state_dict(\"../weights/test/final.pt\")\n",
    "\n",
    "# net.to(torch.device(\"cpu\"))\n",
    "# net = InferenceModel(\"../weights/test/inference.onnx\")\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\n",
    "    \"../weights/simplified.onnx\", providers=[\"CPUExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bca32-dbb7-4bd9-9c7a-ce4582ba044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811a65d-ba65-4c30-abe1-1e9a0ebf0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "\n",
    "device = cuda.Device(0) \n",
    "cuda_context = device.make_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f44ee-68f9-4083-8736-de1d62c8c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your serialized TensorRT engine (model)\n",
    "def load_engine(trt_runtime, engine_path):\n",
    "    with open(engine_path, 'rb') as f:\n",
    "        engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "    return engine\n",
    "\n",
    "# Allocate buffers for input and output\n",
    "def allocate_buffers(engine):\n",
    "\n",
    "    context = engine.create_execution_context()\n",
    "    input_shape = context.get_binding_shape(0)  # Assuming one input. Adjust if your model differs.\n",
    "    output_shape = context.get_binding_shape(1)  # Assuming one output. Adjust if your model differs.\n",
    "    dtype = trt.nptype(engine.get_binding_dtype(0))  # Assuming dtype is the same for input and output\n",
    "    # Allocate host and device buffers\n",
    "    d_input = cuda.mem_alloc(int(np.prod(input_shape) * 4))\n",
    "    d_output = cuda.mem_alloc(int(np.prod(output_shape) * 4))\n",
    "    h_output = np.empty(output_shape, dtype=dtype)\n",
    "    bindings = [int(d_input), int(d_output)]\n",
    "    stream = cuda.Stream()\n",
    "\n",
    "    return d_input, d_output, h_output, bindings, stream, context\n",
    "\n",
    "# Loading the TRT engine\n",
    "engine_path = '../weights/simplified.trt'  # Provide the path to your .trt model file\n",
    "engine = load_engine(trt_runtime, engine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b097785-fa18-4d77-868c-d27c475705d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate buffers\n",
    "d_input, d_output, h_output, bindings, stream, context = allocate_buffers(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498da6f-cc72-41d1-b899-7e735b0e2c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_inference(context, bindings, input_data, d_input, d_output, h_output, stream):\n",
    "    \"\"\"\n",
    "    Execute the model inference.\n",
    "    :param context: The TensorRT execution context.\n",
    "    :param bindings: The bindings for the input and output memory pointers.\n",
    "    :param input_data: The input data for the model.\n",
    "    :param d_input: Device input buffer.\n",
    "    :param d_output: Device output buffer.\n",
    "    :param h_output: Host output buffer.\n",
    "    :param stream: CUDA stream for asynchronous execution.\n",
    "    :return: The model's output data.\n",
    "    \"\"\"\n",
    "    # Transfer input data to the GPU.\n",
    "    cuda.memcpy_htod_async(d_input, input_data, stream)\n",
    "    # Execute the model.\n",
    "    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "    # Transfer predictions back from the GPU.\n",
    "    cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return the host output.\n",
    "    return h_output\n",
    "\n",
    "# Assuming your model expects a single input with shape (1, C, H, W) where C, H, W are channel, height, and width.\n",
    "# Update this to match your model's expected input shape.\n",
    "input_shape = (1, 3, 320, 320)  # Replace C, H, W with actual values.\n",
    "dtype = np.float32  # Replace with the actual dtype your model expects (e.g., np.float16, np.float32)\n",
    "\n",
    "# Generate a dummy input. Replace this with real data when you run your model.\n",
    "input_data = np.random.random_sample(input_shape).astype(dtype)\n",
    "\n",
    "# Prepare input data for the model (ensure it matches the expected input shape and type)\n",
    "input_data = np.ascontiguousarray(input_data)\n",
    "\n",
    "# Perform inference\n",
    "output_data = do_inference(context, bindings, input_data, d_input, d_output, h_output, stream)\n",
    "\n",
    "print(\"Inference output:\", output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50548b-fad9-4705-897a-3cee0b746deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_context.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9dec5a-a736-4054-9102-c40766935358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, targets):\n",
    "    image_size = image.shape[:2]\n",
    "    pad = 0\n",
    "    \n",
    "    for target in targets:\n",
    "        if target[-1] >= 0:\n",
    "            x_min = (\n",
    "                int(\n",
    "                    target[0] * image_size[0]\n",
    "                    - target[2] * image_size[0] / 2\n",
    "                )\n",
    "                + pad\n",
    "                )\n",
    "            y_min = (\n",
    "                int(\n",
    "                    target[1] * image_size[1]\n",
    "                    - target[3] * image_size[1] / 2\n",
    "                )\n",
    "                + pad\n",
    "            )\n",
    "            x_max = pad + int(x_min + target[2] * image_size[0])\n",
    "            y_max = pad + int(y_min + target[3] * image_size[1])\n",
    "\n",
    "            draw_bbox_opencv(\n",
    "                image,\n",
    "                (x_min, y_min, x_max, y_max),\n",
    "                \"person\",\n",
    "                target[4],\n",
    "                color=(255, 0, 0),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ca4e5-0f2e-4714-b2a2-21562c72d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference_onnx(frame, ort_session, cuda=False):\n",
    "    io_binding = ort_session.io_binding()\n",
    "    \n",
    "    device = \"cuda\" if cuda else \"cpu\"\n",
    "\n",
    "    # Adjust the shape of your frame to match the input shape of the model\n",
    "    # This might involve adding a batch dimension or resizing if necessary\n",
    "    # For example, if your model expects a batch dimension:\n",
    "    frame = np.array(frame)\n",
    "    frame = (frame.T / 255.0).astype(np.float32)\n",
    "    frame = np.expand_dims(frame, axis=0)\n",
    "\n",
    "    # Ensure the frame is the expected dtype (e.g., float32)\n",
    "    frame = frame.astype(np.float32)\n",
    "\n",
    "    # Create a tensor from the frame for binding\n",
    "    frame_tensor = onnxruntime.OrtValue.ortvalue_from_numpy(frame, device, 0) # Assuming device_id is 0\n",
    "    output_tensor = onnxruntime.OrtValue.ortvalue_from_shape_and_type([1, 5, 10, 10, 6], np.float32, device, 0)  # Change the shape to the actual shape of the output being bound\n",
    "    # Bind the input tensor to the input name\n",
    "    io_binding.bind_ortvalue_input(\"input\", frame_tensor)\n",
    "    io_binding.bind_ortvalue_output(\"output\", output_tensor)\n",
    "    ort_session.run_with_iobinding(io_binding)\n",
    "    \n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    # Retrieve the outputs\n",
    "    ort_outs = io_binding.copy_outputs_to_cpu()[0]\n",
    "\n",
    "    outputs = torch.tensor(ort_outs)\n",
    "    # print(outputs)\n",
    "    # outputs = filter_boxes_separate(outputs, 0.5, 0.3)\n",
    "    outputs = filter_boxes(outputs, 0.3)\n",
    "    outputs = nms(outputs, 0.25)\n",
    "    outputs = [np.array(output) for output in outputs]\n",
    "    targets = np.stack(outputs, axis=0)[0]\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246d408c-0fe6-4644-97d6-7f81cdb1b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference_torch(frame, net, cuda=False):    \n",
    "    device = \"cuda\" if cuda else \"cpu\"\n",
    "\n",
    "    # Adjust the shape of your frame to match the input shape of the model\n",
    "    # This might involve adding a batch dimension or resizing if necessary\n",
    "    # For example, if your model expects a batch dimension:\n",
    "    frame = np.array(frame)\n",
    "    frame = (frame.T / 255.0).astype(np.float32)\n",
    "    frame = np.expand_dims(frame, axis=0)\n",
    "\n",
    "    # Ensure the frame is the expected dtype (e.g., float32)\n",
    "    frame = frame.astype(np.float32)\n",
    "    frame = torch.tensor(frame)\n",
    "\n",
    "    outputs = net(frame)\n",
    "    \n",
    "    # print(outputs)\n",
    "    # outputs = filter_boxes_separate(outputs, 0.5, 0.3)\n",
    "    outputs = filter_boxes(outputs, 0.1)\n",
    "    outputs = nms(outputs, 0.25)\n",
    "    outputs = [np.array(output.detach().cpu().numpy()) for output in outputs]\n",
    "    targets = np.stack(outputs, axis=0)[0]\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f3687-35db-4853-8e60-ba7e589b84ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference_tensorrt_backend(frame, engine, cuda=False):\n",
    "    # Adjust the shape of your frame to match the input shape of the model\n",
    "    # This might involve adding a batch dimension or resizing if necessary\n",
    "    # For example, if your model expects a batch dimension:\n",
    "    frame = np.array(frame)\n",
    "    frame = (frame.T / 255.0).astype(np.float32)\n",
    "    frame = np.expand_dims(frame, axis=0)\n",
    "\n",
    "    # Ensure the frame is the expected dtype (e.g., float32)\n",
    "    frame = frame.astype(np.float32)\n",
    "\n",
    "    trt_out = engine.run(frame)[0]\n",
    "    \n",
    "    outputs = torch.tensor(trt_out)\n",
    "    # outputs = filter_boxes_separate(outputs, 0.5, 0.3)\n",
    "    # print(outputs)\n",
    "    outputs = filter_boxes(outputs, 0.3)\n",
    "    outputs = nms(outputs, 0.25)\n",
    "    outputs = [np.array(output) for output in outputs]\n",
    "    targets = np.stack(outputs, axis=0)[0]\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1328d6ab-f74d-403d-b8d8-9675beb05024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference_tensorrt(frame):\n",
    "    # Adjust the shape of your frame to match the input shape of the model\n",
    "    # This might involve adding a batch dimension or resizing if necessary\n",
    "    # For example, if your model expects a batch dimension:\n",
    "    frame = np.array(frame)\n",
    "    frame = (frame.T / 255.0).astype(np.float32)\n",
    "    frame = np.expand_dims(frame, axis=0)\n",
    "\n",
    "    # Ensure the frame is the expected dtype (e.g., float32)\n",
    "    frame = frame.astype(np.float32)\n",
    "\n",
    "    # Prepare input data for the model (ensure it matches the expected input shape and type)\n",
    "    input_data = np.ascontiguousarray(frame)\n",
    "\n",
    "    # Perform inference\n",
    "    trt_out = do_inference(context, bindings, input_data, d_input, d_output, h_output, stream)\n",
    "    \n",
    "    outputs = torch.tensor(trt_out)\n",
    "    # outputs = filter_boxes_separate(outputs, 0.5, 0.3)\n",
    "    # print(outputs)\n",
    "    outputs = filter_boxes(outputs, 0.1)\n",
    "    outputs = nms(outputs, 0.25)\n",
    "    outputs = [np.array(output) for output in outputs]\n",
    "    targets = np.stack(outputs, axis=0)[0]\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f273cb05-1fa3-45cf-b637-66a6e22dbea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback function (your detection pipeline)\n",
    "# Make sure to first load all your pipeline code and only at the end init the camera\n",
    "now = time.time()\n",
    "def callback(image):\n",
    "    global now\n",
    "\n",
    "    fps = f\"{int(1/(time.time() - now))}\"\n",
    "    now = time.time()\n",
    "    \n",
    "    crop_to_fill = False\n",
    "    \n",
    "    if crop_to_fill:\n",
    "        image = image[0:320, 0:320, :]\n",
    "    else:\n",
    "        width = 640\n",
    "        height = 360\n",
    "        scale = min(320/width, 320/height)\n",
    "\n",
    "        image = cv2.resize(image, fx=scale, fy=scale, dsize=None)\n",
    "\n",
    "        pad_height = max(0, 320 - scale*height)\n",
    "        top_pad = int(pad_height // 2)\n",
    "        bottom_pad = int(pad_height - top_pad)\n",
    "\n",
    "        # Pad the image\n",
    "        image = cv2.copyMakeBorder(image, top_pad, bottom_pad, 0, 0, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "\n",
    "\n",
    "    targets = perform_inference_torch(image, net, False)    \n",
    "    # targets = perform_inference_onnx(image, ort_session, False)\n",
    "    # targets = perform_inference_tensorrt_backend(image, engine, False)\n",
    "    # targets = perform_inference_tensorrt(image)\n",
    "\n",
    "    draw_boxes(image, targets)\n",
    "   \n",
    "    cv2.putText(image, \"fps=\"+fps, (2, 25), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eda44fb7-e1e3-4ee0-9738-e84dc5b20621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 160, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback(np.zeros((320, 320, 3), np.uint8)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3862254a-5f10-4d50-a109-fae94009e508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing camera...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4c87b4f8f34e569de5ae10303da3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the camera with the callback\n",
    "cam = CameraTensorrtDisplay(engine_path=\"../weights/simplified.trt\", crop_to_fill=False)\n",
    "# cam = CameraDisplay(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef4f134f-6d8e-4e28-b9cd-ea47247856c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The camera stream can be started with cam.start()\n",
    "# The callback gets asynchronously called (can be stopped with cam.stop())\n",
    "# cam.run()\n",
    "cam.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9baf3b9-a78c-4709-a1df-4fabd57366a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera released\n"
     ]
    }
   ],
   "source": [
    "# The camera should always be stopped and released for a new camera is instantiated (calling CameraDisplay(callback) again)\n",
    "#cam.stop()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4106239-8833-473c-b89b-038edd40f5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
