{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blocked-second",
   "metadata": {},
   "source": [
    "# Embedded ML Lab - Challenge (testing yolo example)\n",
    "\n",
    "This is an example of inference with the VOC data set and tinyyolov2. There are pretrained weights (`voc_pretrained.pt`) stored that can be loaded into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "verified-celebrity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "\n",
    "from faf.utils.dataloader import VOCDataLoader\n",
    "loader = VOCDataLoader(train=False, batch_size=1, path=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "devoted-percentage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyYoloV2(\n",
       "  (pad): ReflectionPad2d((0, 1, 0, 1))\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv7): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn8): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): Conv2d(1024, 125, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faf.tinyyolov2 import TinyYoloV2\n",
    "from faf.utils.yolo import nms, filter_boxes\n",
    "from faf.utils.viz import display_result\n",
    "\n",
    "# make an instance with 20 classes as output\n",
    "net = TinyYoloV2(num_classes=20)\n",
    "\n",
    "# load pretrained weights\n",
    "sd = torch.load(\"./weights/voc_pretrained.pt\")\n",
    "net.load_state_dict(sd)\n",
    "\n",
    "#put network in evaluation mode\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ee25ea-29b1-4fd6-8206-3e31569457f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 10, 10, 6])\n",
      "torch.Size([1, 5, 10, 10, 25])\n"
     ]
    }
   ],
   "source": [
    "import faf.person_only as fpo\n",
    "import importlib\n",
    "importlib.reload(fpo)\n",
    "net.state_dict().keys()\n",
    "\n",
    "val = next(iter(loader))\n",
    "\n",
    "new_net = fpo.strip_classes(net)\n",
    "print(new_net(val[0]).shape)\n",
    "print(net(val[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "for idx, (input, target) in tqdm.tqdm(enumerate(loader), total=len(loader)):\n",
    "    \n",
    "    #input is a 1 x 3 x 320 x 320 image\n",
    "    output = net(input)\n",
    "    \"output is of a tensor of size 32 x 125 x 10 x 10\"\n",
    "    #output is a 32 x 125 x 10 x 10 tensor\n",
    "    \n",
    "    #filter boxes based on confidence score (class_score*confidence)\n",
    "    output = filter_boxes(output, 0.1)\n",
    "    \n",
    "    #filter boxes based on overlap\n",
    "    output = nms(output, 0.25)\n",
    "    \n",
    "    display_result(input, output, target, file_path='yolo_prediction.png')\n",
    "\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbd66c-cae7-476e-af89-24acfa2b5051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
