import torch
from torchinfo import summary
import os, json

import torchvision
from torchvision import transforms as tf

CLASSES = (
    "aeroplane",
    "bicycle",
    "bird",
    "boat",
    "bottle",
    "bus",
    "car",
    "cat",
    "chair",
    "cow",
    "diningtable",
    "dog",
    "horse",
    "motorbike",
    "person",
    "pottedplant",
    "sheep",
    "sofa",
    "train",
    "tvmonitor",
)


def class_to_num(class_str):
    for idx, string in enumerate(CLASSES):
        if string == class_str:
            return idx


def num_to_class(number):
    for idx, string in enumerate(CLASSES):
        if idx == number:
            return string
    return "none"


class VOCTransform:
    def __init__(self, train=True, only_person=False):
        self.only_person = only_person
        self.train = train
        if train:
            self.augmentation = tf.RandomApply([tf.ColorJitter(0.2, 0.2, 0.2, 0.2)])

    def __call__(self, image, target):
        num_bboxes = 10
        width, height = 320, 320

        img_width, img_height = image.size

        scale = min(width / img_width, height / img_height)
        new_width, new_height = int(img_width * scale), int(img_height * scale)

        diff_width, diff_height = width - new_width, height - new_height
        image = tf.functional.resize(image, size=(new_height, new_width))
        image = tf.functional.pad(
            image,
            padding=(
                diff_width // 2,
                diff_height // 2,
                diff_width // 2 + diff_width % 2,
                diff_height // 2 + diff_height % 2,
            ),
        )
        target = target["annotation"]["object"]

        target_vectors = []
        for item in target:
            x0 = int(item["bndbox"]["xmin"]) * scale + diff_width // 2
            w = (int(item["bndbox"]["xmax"]) - int(item["bndbox"]["xmin"])) * scale
            y0 = int(item["bndbox"]["ymin"]) * scale + diff_height // 2
            h = (int(item["bndbox"]["ymax"]) - int(item["bndbox"]["ymin"])) * scale

            target_vector = [
                (x0 + w / 2) / width,
                (y0 + h / 2) / height,
                w / width,
                h / height,
                1.0,
                class_to_num(item["name"]),
            ]

            if self.only_person:
                if target_vector[5] == class_to_num("person"):
                    target_vector[5] = 0.0
                    target_vectors.append(target_vector)
            else:
                target_vectors.append(target_vector)

        target_vectors = list(sorted(target_vectors, key=lambda x: x[2] * x[3]))
        target_vectors = torch.tensor(target_vectors)
        if target_vectors.shape[0] < num_bboxes:
            zeros = torch.zeros((num_bboxes - target_vectors.shape[0], 6))
            zeros[:, -1] = -1
            target_vectors = torch.cat([target_vectors, zeros], 0)
        elif target_vectors.shape[0] > num_bboxes:
            target_vectors = target_vectors[:num_bboxes]

        if self.train:
            return (
                self.augmentation(tf.functional.to_tensor(image)),
                target_vectors,
            )
        else:
            return tf.functional.to_tensor(image), target_vectors


def VOCDataLoader(train=True, batch_size=32, shuffle=False, path=None):
    if path is None:
        path = "data/"
    if train:
        image_set = "train"
    else:
        image_set = "val"

    if not os.path.exists(
        os.path.join(path, "VOCdevkit/VOC2012/JPEGImages/2007_000027.jpg")
    ):
        dataset = torchvision.datasets.VOCDetection(
            path, year="2012", image_set=image_set, download=True
        )

    dataset = torchvision.datasets.VOCDetection(
        path,
        year="2012",
        image_set=image_set,
        download=False,
        transforms=VOCTransform(train=train),
    )
    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


def VOCDataLoaderPerson(train=True, batch_size=32, shuffle=False, path=None):
    if path is None:
        path = "data/"
    if train:
        image_set = "train"
    else:
        image_set = "val"

    if not os.path.exists(
        os.path.join(path, "VOCdevkit/VOC2012/JPEGImages/2007_000027.jpg")
    ):
        dataset = torchvision.datasets.VOCDetection(
            path, year="2012", image_set=image_set, download=True
        )

    dataset = torchvision.datasets.VOCDetection(
        path,
        year="2012",
        image_set=image_set,
        download=False,
        transforms=VOCTransform(train=train, only_person=True),
    )
    with open(os.path.join(path, "person_indices.json"), "r") as fd:
        indices = list(json.load(fd)[image_set])
    dataset = torch.utils.data.Subset(dataset, indices)
    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
